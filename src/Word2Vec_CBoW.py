# -*- coding: utf-8 -*-
"""NLP Assignment 1 (Q2)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FcbVh55wEgYD1Ta_ekVZuo21xD0A5H6k
"""

# Import libraries
from gensim.models import Word2Vec
import numpy as np
import json
import re

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam
from torch.utils.data import Dataset, DataLoader

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import matplotlib.cm as cm
% matplotlib inline

!pip install nltk==3.6.2
import nltk
nltk.download('punkt')

from nltk import ngrams
from nltk.tokenize import word_tokenize, sent_tokenize

print(nltk.__version__)

# Paths
PATH = "./dataset/Electronics_5.json"
model_path = "./outputs/Q2/model.pt"
vocab_path = "./outputs/Q2/vocab.pt"
embed_path = "./outputs/Q2/embed.npy"
plot_path = './outputs/Q2/plot.png'

print("Starting !!!")

"""# 16,89,188 sentences"""

# Method to preprocess the input sentence
def preprocess(sentence):
    if not sentence.isspace():
        return re.sub("[^a-zA-Z0-9 ]", " ", sentence).lower().strip()

class Sentence():
    '''
    Sentence Generator class
    '''

    # Init method - initialize object with corpus file path and corpus limit size
    def __init__(self, PATH, lim):
        self.PATH = PATH
        self.lim = lim

    # Generator method - yields one sentence at a time
    def __iter__(self):
        for i, line in enumerate(open(self.PATH)):
            if i >= self.lim:
                break
            para = json.loads(line)['reviewText']
            for sent in sent_tokenize(para):
                yield ['SOS'] + preprocess(sent).split() + ['EOS']


class Vocabulary():
    '''
    Vocabulary class
    '''

    # Init method - initialize object with various mappings
    def __init__(self, sentences, freqlimit):
        self.word2ind = {
            'UNK': 0,
            'SOS': 1,
            'EOS': 2
        }
        self.ind2word = {
            0: 'UNK',
            1: 'SOS',
            2: 'EOS'
        }
        self.word2freq = {
            'UNK': 0,
            'SOS': 0,
            'EOS': 0
        }
        self.vocabsize = 3

        self.add_corpus(sentences)
        self.delete_low_freq(freqlimit)
        self.prepare_vocab()

    # Method to iteratively add corpus sentences
    def add_corpus(self, sentences):
        for sent in sentences:
            for word in sent:
                self.add_word(word)

    # Method to add word into frequency mapping
    def add_word(self, word):
        if word not in self.word2freq:
            self.word2freq[word] = 1
        else:
            self.word2freq[word] += 1

    # Method to delete low frequency words from frequency mapping
    def delete_low_freq(self, freqlimit):
        delete_keys = [key for key in self.word2freq if self.word2freq[key] < freqlimit]

        for key in delete_keys:
            del self.word2freq[key]

    # Method to prepare vocabulary with word2ind and ind2word mappings
    def prepare_vocab(self):
        for word in self.word2freq:
            if word not in self.word2ind:
                self.word2ind[word] = self.vocabsize
                self.ind2word[self.vocabsize] = word
                self.vocabsize += 1

    # Getter method - return integer mapping of input word
    def get_ind(self, word):
        if word not in self.word2ind:
            return self.word2ind['UNK']
        return self.word2ind[word]

    # Getter method - return word corresponding to input integer
    def get_word(self, ind):
        if ind not in self.ind2word:
            return self.ind2word[0]
        return self.ind2word[ind]

    # Getter method - return frequency of the input word
    def get_freq(self, word):
        if word not in self.word2freq:
            return self.word2freq['UNK']
        return self.word2freq[word]


"""15,52,671 words in vocab"""


class CbowDataset(Dataset):
    '''
    Continuous Bag of Words Dataset class
    '''

    # Init class - initializes object with contexts and targets
    def __init__(self):
        self.context = []
        self.target = []
        self.onehotlen = 0

    # Method to prepare data given sentences
    def prepare_data(self, sentences, vocab, window):
        for sent in sentences:
            tokens = list(filter(lambda i: i in vocab.word2ind, sent))
            nGrams = ngrams(tokens, 2*window+1)

            for ngram in nGrams:
                ngram = list(ngram)
                self.target.append(vocab.get_ind(ngram.pop(window)))
                self.context.append(torch.from_numpy(np.asarray([vocab.get_ind(word) for word in ngram], dtype=int)))
        
        self.onehotlen = vocab.vocabsize

    # Getter method - return one hot encoded vector of the input word
    def __getitem__(self, idx):
        return F.one_hot(self.context[idx], self.onehotlen).float(), self.target[idx]

    # Getter method - return length of dataset (CBoWs)
    def __len__(self):
        return len(self.target)


class CustomWord2Vec(nn.Module):
    '''
    Custom Word2Vec Model class
    '''

    # Init method - initializes object with vocab and embedding size
    def __init__(self, vocab, embedding_size):
        super(CustomWord2Vec, self).__init__()
        self.vocab = vocab
        self.embed_layer = torch.nn.Linear(vocab.vocabsize, embedding_size, bias=False)
        self.output_layer = torch.nn.Linear(embedding_size, vocab.vocabsize, bias=False)

    # Method for forward pass
    def forward(self, x):
        out = self.embed_layer(x.float())
        out = torch.mean(out, dim=1)
        return self.output_layer(out)
    
    # Method to predict the target word given context
    def predict(self, x):
        out = self.forward(x)
        out = F.softmax(out, dim=1)
        return torch.argmax(out, dim=1)

    # Getter method - return word embedding of input word
    def get_embedding(self, word):
        one_hot = F.one_hot(torch.from_numpy(np.array(self.vocab.get_ind(word))), self.vocab.vocabsize).float().cuda()
        return self.embed_layer(one_hot).detach()

    # Getter method - return top n similar words of input word
    def similar_words(self, word, top):
        word_embed = self.get_embedding(word)
        cos_dist = 1 - torch.matmul(self.embed_layer.weight.T, word_embed)/(torch.norm(self.embed_layer.weight.T, dim=1)*torch.norm(word_embed))

        sorted_inds = torch.argsort(cos_dist)[:top]
        return [(self.vocab.get_word(ind.item()), 1-float(cos_dist[ind].detach().cpu())) for ind in sorted_inds]


# Function to train the model
def training(net, loader, epochs=10, lr=0.001):

    print('Training started !!')

    opt = torch.optim.Adam(net.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    net.cuda()
    net.train()

    for ep in range(epochs):

        print('Epoch %d starting' % (ep+1))

        ep_loss = 0
        for i, data in enumerate(loader):
            contexts, targets = data
            
            contexts = contexts.cuda()
            targets = targets.cuda()

            opt.zero_grad()

            outputs = net.forward(contexts)
            loss = criterion(outputs, targets)
            loss.backward()
            opt.step()

            ep_loss += loss.item()

            if (i+1) % 5000 == 0:
                print('Epoch: %2d\tBatch: %5d\tTrain Loss: %.3f' % (ep+1, i+1, ep_loss/(i+1)))

        ep_loss /= len(loader)

        print('Epoch %d completed' % (ep+1))

        print('Epoch: %d\tTrain Loss: %.3f' % (ep+1, ep_loss))

    print('Training completed !!')


# Method to create embedding clusters list and word clusters list
def get_clusters(keywords, model):
    embedding_clusters = []
    word_clusters = []

    for word in keywords:
        embeddings = []
        words = []

        for similar_word, _ in model.similar_words(word, 11):
            words.append(similar_word)
            embeddings.append(np.array(model.get_embedding(similar_word).cpu()))
        
        embedding_clusters.append(embeddings)
        word_clusters.append(words)
    
    embedding_clusters = np.array(embedding_clusters)
    return embedding_clusters, word_clusters

# Function to plot the t-SNE plot given the clusters information
def tsne_plot_similar_words(title, labels, embedding_clusters, word_clusters, filename=None):
    plt.figure(figsize=(16, 9))
    colors = cm.rainbow(np.linspace(0, 1, len(labels)))
    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):
        x = embeddings[1:, 0]
        y = embeddings[1:, 1]

        plt.scatter(embeddings[0, 0], embeddings[0, 1], c="black", alpha=0.7)
        plt.annotate(words[0], alpha=1, xy=(embeddings[0, 0], embeddings[0, 1]), xytext=(5, 2),
                         textcoords='offset points', ha='right', va='bottom', size=8)
        
        plt.scatter(x, y, c=color, alpha=0.7, label=label)
        for i, word in enumerate(words[1:]):
            plt.annotate(word, alpha=0.7, xy=(x[i], y[i]), xytext=(5, 2),
                         textcoords='offset points', ha='right', va='bottom', size=8)
    plt.legend(loc=4)
    plt.title(title)
    plt.grid(True)
    if filename:
        plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')
    plt.show()

# Function to plot the t-SNE plot
def plot_tsne(keywords, model):
    embed_clusters, word_clusters = get_clusters(keywords, model)

    n, m, k = embed_clusters.shape
    tsne_model_2d = TSNE(perplexity=10, n_components=2, init='pca', n_iter=10000, random_state=16)
    embeddings_2d = np.array(tsne_model_2d.fit_transform(embed_clusters.reshape(n * m, k))).reshape(n, m, 2)

    tsne_plot_similar_words('Similar words from Electronics Review Dataset', keywords, embeddings_2d, word_clusters, plot_path)


epochs = 25
lr = 0.001

sentences = Sentence(PATH, 100000)
Vocab = Vocabulary(sentences, 5)

train_dataset = CbowDataset()
train_dataset.prepare_data(sentences, Vocab, 2)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

model = CustomWord2Vec(Vocab, 50)
model.cuda()

training(model, train_loader, epochs, lr)

print("Top 10 similar words obtained by Word Embedding Model for the word camera are,", model.similar_words('camera', 10))

torch.save(model, model_path)
torch.save(Vocab, vocab_path)
np.save(embed_path, model.embed_layer.weight.T.detach().cpu())

keywords = ['they', 'computer', 'good', 'sleep', 'amazon']
plot_tsne(keywords, model)